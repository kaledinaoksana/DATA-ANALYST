{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6 - Data Sourcing via Web\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA\n",
    "\n",
    "1. What is the difference between stemming and lemmatization?\n",
    "\n",
    "Stemming reduces words to its root word and lemmatization reduces words to its base word.\n",
    "Stemming is the process of reducing the inflectional forms of each word to a common root word and lemmatizing is process of reducing the inflectional forms of each word to its base word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from bs4 import BeautifulSoup as bs4\n",
    "import pandas as pd\n",
    "\n",
    "import urllib\n",
    "import urllib.request\n",
    "import re\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment 1 - Objects in BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BeautifulSoup objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/parsing1.txt', 'r') as file:\n",
    "    our_html_document = file.read()\n",
    "\n",
    "soup = bs4(our_html_document, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify()[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag objects\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tag names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = soup.h1\n",
    "print(tag)\n",
    "type(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tag)\n",
    "print(tag.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag.name = 'heading 1'\n",
    "print(tag)\n",
    "print(tag.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tag attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_atr = bs4('<h1 attribute_1 = \"Heading Level 1\"\">Future Trends for IoT in 2018</h1>', 'html.parser')\n",
    "tag = soup_atr.h1\n",
    "tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag['attribute_1']\n",
    "print(tag.attrs)\n",
    "tag['attribute_2'] = 'Heading Level 1*'\n",
    "print(tag.attrs)\n",
    "print(tag)\n",
    "del tag['attribute_2']\n",
    "print(tag)\n",
    "del tag['attribute_1']\n",
    "print(tag.attrs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Navigating a parse tree using tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('data/parsing2.txt', 'r') as file:\n",
    "    our_html_document = file.read()\n",
    "\n",
    "sp = bs4(our_html_document, 'html.parser')\n",
    "\n",
    "print(sp.head,'\\n')\n",
    "print(sp.title,'\\n')\n",
    "print(sp.body.b,'\\n')\n",
    "#print(sp.body,'\\n')\n",
    "print(sp.li,'\\n')\n",
    "print(sp.a,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment 2 - NavigatableString Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NavigatableString objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_object = bs4('<h1 attribute_1 = \"Heading Level 1\"\">Future Trends in IoT in 2018</h1>', \"html.parser\")\n",
    "tag = soup_object.h1\n",
    "print(type(tag))\n",
    "print(tag.name)\n",
    "print(tag.string)\n",
    "print(type(tag.string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_navigatable_string = tag.string\n",
    "print(our_navigatable_string)\n",
    "\n",
    "our_navigatable_string.replace_with('NaN')\n",
    "print(tag.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilizing NavigatableString objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/parsing3.txt', 'r') as file:\n",
    "    our_html_document = file.read()\n",
    "\n",
    "sp = bs4(our_html_document, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for string in sp.stripped_strings:\n",
    "    #print(repr(string))\n",
    "    a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_link = sp.a\n",
    "print(first_link)\n",
    "print(first_link.parent)\n",
    "print(first_link.string)\n",
    "print(first_link.string.parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment 3 - Data parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/parsing4.txt', 'r') as file:\n",
    "    our_html_document = file.read()\n",
    "\n",
    "soup = bs4(our_html_document, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify()[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_only = soup.get_text()\n",
    "#print(text_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(\"li\")\n",
    "soup.find_all(id=\"link 7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soup.find_all('ol')\n",
    "#soup.find_all(['ol', 'b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = re.compile(\"t\")\n",
    "for tag in soup.find_all(t):\n",
    "    print(tag.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in soup.find_all(True):\n",
    "    print(tag.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(string=re.compile(\"data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment 4 - Web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs4\n",
    "\n",
    "url = 'https://analytics.usa.gov'\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36\", \n",
    "            \"Accept-Encoding\":\"gzip, deflate, br\", \n",
    "            \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\", \n",
    "            \"DNT\":\"1\",\n",
    "            \"Connection\":\"close\", \n",
    "            \"Upgrade-Insecure-Requests\":\"1\"\n",
    "            }\n",
    "\n",
    "def connect(URL):\n",
    "    page = requests.get(URL, headers=headers)\n",
    "    soup1 = bs4(page.content, \"html.parser\")\n",
    "    return bs4(soup1.prettify(), 'html.parser')\n",
    "\n",
    "soup = connect(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(soup.get_text())\n",
    "#print(soup.prettify()[0:1000])\n",
    "for link in soup.findAll('a', attrs={'href': re.compile(\"^http\")}):\n",
    "    print(link)\n",
    "type(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"data/save_files/parsed_data.txt\", \"w\")\n",
    "for link in soup.findAll('a', attrs={'href': re.compile(\"^http\")}):\n",
    "    soup_link = str(link)\n",
    "    print(soup_link)\n",
    "    file.write(soup_link)\n",
    "file.flush()\n",
    "file.close()\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import certifi\n",
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment 5 - Introduction to NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import pos_tag\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"On Wednesday, the Association for Computing Machinery, the world's largest society of computing professionals, announced that Hinton, LeCun and Bengio had won this year's Turing Award for their work on neural networks. The Turing Award, which was introduced in 1966, is often called the Nobel Prize of computing, and it includes a $1 million prize, which the three scientists will share.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tk = sent_tokenize(text)\n",
    "print(\"Sentence tokenizing the text: \\n\")\n",
    "print(sent_tk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tk = word_tokenize(text)\n",
    "print(\"Word tokenizing the text: \\n\")\n",
    "print(word_tk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and stemming textual data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = set(stopwords.words(\"english\"))\n",
    "print(\"Stop words in English language are: \\n\")\n",
    "print(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_ru= set(stopwords.words(\"russian\"))\n",
    "print(\"Stop words in Russian language are: \\n\")\n",
    "print(sw_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_words = [w for w in word_tk if not w in sw]\n",
    "print(\"The text after removing stop words \\n\")\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "port_stem = PorterStemmer()\n",
    "stemmed_words = []\n",
    "\n",
    "for w in filtered_words:\n",
    "    stemmed_words.append(port_stem.stem(w))\n",
    "    \n",
    "print(\"Filtered Sentence: \\n\", filtered_words, \"\\n\")\n",
    "print(\"Stemmed Sentence: \\n\", stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing and Analysing textual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "stem = PorterStemmer()\n",
    "lemm_words = []\n",
    "\n",
    "for i in range(len(filtered_words)):\n",
    "    lemm_words.append(lem.lemmatize(filtered_words[i]))\n",
    "\n",
    "print(filtered_words,'\\n')    \n",
    "print(lemm_words,'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parts of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "pos_tagged_words = pos_tag(word_tk)\n",
    "print(pos_tagged_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency Distribution Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "import seaborn as sb\n",
    "\n",
    "%matplotlib inline\n",
    "rcParams['figure.figsize'] = 6,5\n",
    "sb.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = FreqDist(word_tk)\n",
    "print(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd.plot(30, cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd_alpha = FreqDist(text)\n",
    "print(fd_alpha)\n",
    "fd_alpha.plot(30, cumulative=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "num-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
