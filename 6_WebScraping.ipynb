{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6 - Data Sourcing via Web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment 1 - Objects in BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import sys\n",
    "print(sys.version)\n",
    "from bs4 import BeautifulSoup\n",
    "### BeautifulSoup objects \n",
    "our_html_document = # parsing1\n",
    "our_soup_object = BeautifulSoup(our_html_document, 'html.parser')\n",
    "print(our_soup_object)\n",
    "print(our_soup_object.prettify()[0:300])\n",
    "### Tag objects\n",
    "#### Tag names\n",
    "soup_object = BeautifulSoup('<h1 attribute_1 = \"Heading Level 1\"\">Future Trends for IoT in 2018</h1>', \"lxml\")\n",
    "\n",
    "tag = soup_object.h1\n",
    "type(tag)\n",
    "print(tag)\n",
    "tag.name\n",
    "tag.name = 'heading 1'\n",
    "tag\n",
    "tag.name\n",
    "#### Tag attributes\n",
    "soup_object = BeautifulSoup('<h1 attribute_1 = \"Heading Level 1\"\">Future Trends for IoT in 2018</h1>', \"lxml\")\n",
    "tag = soup_object.h1\n",
    "tag\n",
    "tag['attribute_1']\n",
    "tag.attrs\n",
    "tag['attribute_2'] = 'Heading Level 1*'\n",
    "tag.attrs\n",
    "tag\n",
    "del tag['attribute_2']\n",
    "tag\n",
    "del tag['attribute_1']\n",
    "tag.attrs\n",
    "#### Navigating a parse tree using tags\n",
    "# First we will recreate our original parse tree.\n",
    "our_html_document = # parsing2\n",
    "our_soup_object = BeautifulSoup(our_html_document, 'html.parser')\n",
    "our_soup_object.head\n",
    "our_soup_object.title\n",
    "our_soup_object.body.b\n",
    "our_soup_object.body\n",
    "our_soup_object.li\n",
    "our_soup_object.a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment 2 - NavigatableString Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chapter 6 - Data Sourcing via Web\n",
    "\n",
    "import sys\n",
    "print(sys.version)\n",
    "from bs4 import BeautifulSoup\n",
    "### NavigatableString objects\n",
    "soup_object = BeautifulSoup('<h1 attribute_1 = \"Heading Level 1\"\">Future Trends in IoT in 2018</h1>', \"lxml\")\n",
    "\n",
    "tag = soup_object.h1\n",
    "\n",
    "type(tag)\n",
    "tag.name\n",
    "tag.string\n",
    "type(tag.string)\n",
    "our_navigatable_string = tag.string\n",
    "our_navigatable_string\n",
    "our_navigatable_string.replace_with('NaN')\n",
    "tag.string\n",
    "#### Utilizing NavigatableString objects\n",
    "our_html_document = # parsing3\n",
    "\n",
    "our_soup_object = BeautifulSoup(our_html_document, 'html.parser')\n",
    "for string in our_soup_object.stripped_strings:\n",
    "    print(repr(string))\n",
    "first_link= our_soup_object.a\n",
    "print(first_link)\n",
    "first_link.parent\n",
    "first_link.string\n",
    "first_link.string.parent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment 3 - Data parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chapter 6 -  Data Sourcing via Web\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import urllib\n",
    "import urllib.request\n",
    "import re\n",
    "with urllib.request.urlopen('https://raw.githubusercontent.com/BigDataGal/Data-Mania-Demos/master/IoT-2018.html') as response:\n",
    "    html = response.read()\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "type(soup)\n",
    "### Parsing your data\n",
    "print(soup.prettify()[0:100])\n",
    "### Getting data from a parse tree\n",
    "text_only = soup.get_text()\n",
    "print(text_only)\n",
    "### Searching and retrieving data from a parse tree\n",
    "#### Retrieving tags by filtering with name arguments\n",
    "soup.find_all(\"li\")\n",
    "##### Retrieving tags by filtering with keyword arguments\n",
    "soup.find_all(id=\"link 7\")\n",
    "##### Retrieving tags by filtering with string arguments\n",
    "soup.find_all('ol')\n",
    "##### Retrieving tags by filtering with list objects\n",
    "soup.find_all(['ol', 'b'])\n",
    "##### Retrieving tags by filtering with regular expressions\n",
    "t = re.compile(\"t\")\n",
    "for tag in soup.find_all(t):\n",
    "    print(tag.name)\n",
    "##### Retrieving tags by filtering with a Boolean value\n",
    "for tag in soup.find_all(True):\n",
    "    print(tag.name)\n",
    "##### Retrieving weblinks by filtering with string objects\n",
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))\n",
    "##### Retrieving strings by filtering with regular expressions\n",
    "soup.find_all(string=re.compile(\"data\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment 4 - Web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chapter 6 -  Data Sourcing via Web\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "from IPython.display import HTML\n",
    "import re\n",
    "r = urllib.request.urlopen('https://analytics.usa.gov/').read()\n",
    "soup = BeautifulSoup(r, \"lxml\")\n",
    "type(soup)\n",
    "print(soup.prettify()[:100])\n",
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))\n",
    "print(soup.get_text())\n",
    "print(soup.prettify()[0:1000])\n",
    "for link in soup.findAll('a', attrs={'href': re.compile(\"^http\")}):\n",
    "    print(link)\n",
    "type(link)\n",
    "file = open(\"parsed_data.txt\", \"w\")\n",
    "for link in soup.findAll('a', attrs={'href': re.compile(\"^http\")}):\n",
    "    soup_link = str(link)\n",
    "    print(soup_link)\n",
    "    file.write(soup_link)\n",
    "file.flush()\n",
    "file.close()\n",
    "%pwd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment 5 - Introduction to NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chapter 6 -  Data Sourcing via Web\n",
    "## Segment 5 - Introduction to NLP\n",
    "import nltk\n",
    "text = \"On Wednesday, the Association for Computing Machinery, the world’s largest society of computing professionals, announced that Hinton, LeCun and Bengio had won this year’s Turing Award for their work on neural networks. The Turing Award, which was introduced in 1966, is often called the Nobel Prize of computing, and it includes a $1 million prize, which the three scientists will share.\"\n",
    "nltk.download('punkt')\n",
    "<h3>Sentence Tokenizer</h3>\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tk = sent_tokenize(text)\n",
    "print(\"Sentence tokenizing the text: \\n\")\n",
    "print(sent_tk)\n",
    "### Word Tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tk = word_tokenize(text)\n",
    "print(\"Word tokenizing the text: \\n\")\n",
    "print(word_tk)\n",
    "### Removing stop words\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw = set(stopwords.words(\"english\"))\n",
    "print(\"Stop words in English language are: \\n\")\n",
    "print(sw)\n",
    "filtered_words = [w for w in word_tk if not w in sw]\n",
    "\n",
    "print(\"The text after removing stop words \\n\")\n",
    "print(filtered_words)\n",
    "<h3>Stemming</h3>\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "port_stem = PorterStemmer()\n",
    "stemmed_words = []\n",
    "\n",
    "for w in filtered_words:\n",
    "    stemmed_words.append(port_stem.stem(w))\n",
    "    \n",
    "print(\"Filtered Sentence: \\n\", filtered_words, \"\\n\")\n",
    "print(\"Stemmed Sentence: \\n\", stemmed_words)\n",
    "# Lemmatizing\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stem = PorterStemmer()\n",
    "\n",
    "lemm_words = []\n",
    "\n",
    "for i in range(len(filtered_words)):\n",
    "    lemm_words.append(lem.lemmatize(filtered_words[i]))\n",
    "    \n",
    "print(lemm_words)\n",
    "<h3>Parts of Speech Tagging</h3>\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import pos_tag\n",
    "pos_tagged_words = pos_tag(word_tk)\n",
    "\n",
    "print(pos_tagged_words)\n",
    "<h3>Frequency Distribution Plots</h3>\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "fd = FreqDist(word_tk)\n",
    "print(fd)\n",
    "import matplotlib.pyplot as plt\n",
    "fd.plot(30, cumulative=False)\n",
    "plt.show()\n",
    "fd_alpha = FreqDist(text)\n",
    "print(fd_alpha)\n",
    "fd_alpha.plot(30, cumulative=False)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
